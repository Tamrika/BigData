{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Topic modelling with Apache Spark and SparkNLP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPbTnlhCQ0oVg/VciEaAdjy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tamrika/BigData/blob/main/Topic_modelling_with_Apache_Spark_and_SparkNLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74ikXFm6rz0Y"
      },
      "source": [
        "# Topic modelling with Apache Spark and SparkNLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FNEyyPoZgXe"
      },
      "source": [
        "## Installing Java and Spark NLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7KDjmgmmYx-h",
        "outputId": "4ce302cb-48a0-4f7e-9733-0ef550bbc61f"
      },
      "source": [
        "import os\n",
        "# Install java\n",
        "! apt-get update -qq\n",
        "! apt-get install -y openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\"+ os.environ[\"PATH\"]\n",
        "! java -version\n",
        "\n",
        "# Install pyspark\n",
        "! pip install --ignore-installed pyspark==2.4.4\n",
        "\n",
        "# Install Spark NLP\n",
        "! pip install --ignore-installed spark-nlp==2.6.3"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "openjdk version \"1.8.0_282\"\n",
            "OpenJDK Runtime Environment (build 1.8.0_282-8u282-b08-0ubuntu1~18.04-b08)\n",
            "OpenJDK 64-Bit Server VM (build 25.282-b08, mixed mode)\n",
            "Processing /root/.cache/pip/wheels/ab/09/4d/0d184230058e654eb1b04467dbc1292f00eaa186544604b471/pyspark-2.4.4-py2.py3-none-any.whl\n",
            "Collecting py4j==0.10.7\n",
            "  Using cached https://files.pythonhosted.org/packages/e3/53/c737818eb9a7dc32a7cd4f1396e787bd94200c3997c72c1dbe028587bd76/py4j-0.10.7-py2.py3-none-any.whl\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.7 pyspark-2.4.4\n",
            "Collecting spark-nlp==2.6.3\n",
            "  Using cached https://files.pythonhosted.org/packages/84/84/3f15673db521fbc4e8e0ec3677a019ba1458b2cb70f0f7738c221511ef32/spark_nlp-2.6.3-py2.py3-none-any.whl\n",
            "Installing collected packages: spark-nlp\n",
            "Successfully installed spark-nlp-2.6.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coGU9tkbZ65K"
      },
      "source": [
        "## Import the relevant packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-ry0aOuaMMP",
        "outputId": "43a19fd1-34e3-41df-a28a-f8c36911f65e"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "from sparknlp.base import *\n",
        "from sparknlp.annotator import *\n",
        "from sparknlp.pretrained import PretrainedPipeline\n",
        "import sparknlp\n",
        "\n",
        "spark = sparknlp.start()\n",
        "\n",
        "print(\"Spark NLP version: \", sparknlp.version())\n",
        "print(\"Apache Spark version: \", spark.version)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Spark NLP version:  2.6.3\n",
            "Apache Spark version:  2.4.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ajmObIM8cYom",
        "outputId": "e6054dcd-ae58-42fb-aeea-f505514ca1e8"
      },
      "source": [
        "from pathlib import Path\n",
        "import urllib.request\n",
        "download_path = \"./abcnews-date-text.csv\"\n",
        "if not Path(download_path).is_file():\n",
        "  print(\"File Not found will downloading it!\")\n",
        "  url = \"https://github.com/ravishchawla/topic_modeling/raw/master/data/abcnews-date-text.csv\"\n",
        "  urllib.request.urlretrieve(url, download_path)\n",
        "else:\n",
        "  print(\"File already present.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "File already present.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtZz5pchaFcO"
      },
      "source": [
        "## Download the news data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EvGkqb6xc8J6",
        "outputId": "6a63fb07-ede2-4e97-e455-94258716286b"
      },
      "source": [
        "# if you are reading file from local storage \n",
        "file_location = r'./abcnews-date-text.csv'\n",
        "# if you are reading file from hdfs\n",
        "# file_location = r'hdfs:\\\\\\user\\path\\to\\abcnews_date_txt.csv'\n",
        "file_type = \"csv\"\n",
        "# CSV options\n",
        "infer_schema = \"true\"\n",
        "first_row_is_header = \"true\"\n",
        "delimiter = \",\"\n",
        "df = spark.read.format(file_type)\\\n",
        "      .option(\"inferSchema\", infer_schema)\\\n",
        "      .option(\"header\", first_row_is_header)\\\n",
        "      .option(\"sep\", delimiter)\\\n",
        "      .load(file_location)\n",
        "# Verify the count\n",
        "df.count()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1041793"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XIruV4mXfYi8",
        "outputId": "87f65e7d-a6db-4638-e4b5-905993b6b17a"
      },
      "source": [
        "df.show(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------+--------------------+\n",
            "|publish_date|       headline_text|\n",
            "+------------+--------------------+\n",
            "|    20030219|aba decides again...|\n",
            "|    20030219|act fire witnesse...|\n",
            "|    20030219|a g calls for inf...|\n",
            "|    20030219|air nz staff in a...|\n",
            "|    20030219|air nz strike to ...|\n",
            "|    20030219|ambitious olsson ...|\n",
            "|    20030219|antic delighted w...|\n",
            "|    20030219|aussie qualifier ...|\n",
            "|    20030219|aust addresses un...|\n",
            "|    20030219|australia is lock...|\n",
            "+------------+--------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZLnpMM7fzeO"
      },
      "source": [
        "## Pre-processing Pipeline using Spark NLP(Assignment 1-4)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMcPTV7ufbZh",
        "outputId": "a92138c5-8029-4a4e-8447-a2436233190c"
      },
      "source": [
        "#3.6.1Document Assembling\n",
        "document_assembler = DocumentAssembler() \\\n",
        "                      .setInputCol(\"headline_text\") \\\n",
        "                      .setOutputCol(\"document\") \\\n",
        "                      .setCleanupMode(\"shrink\")\n",
        "\n",
        "#Split sentence to tokens(array)\n",
        "tokenizer = Tokenizer() \\\n",
        "              .setInputCols([\"document\"]) \\\n",
        "              .setOutputCol(\"token\")\n",
        "#3.6.3 Normalizing-Clean unwanted characters and garbage\n",
        "normalizer = Normalizer() \\\n",
        "              .setInputCols([\"token\"]) \\\n",
        "              .setOutputCol(\"normalized\")\n",
        "#3.6.4 Stopwords removal\n",
        "stopwords_cleaner = StopWordsCleaner()\\\n",
        "                      .setInputCols(\"normalized\")\\\n",
        "                      .setOutputCol(\"cleanTokens\")\\\n",
        "                      .setCaseSensitive(False) \n",
        "#3.6.5 Stemming\n",
        "stemmer = Stemmer() \\\n",
        "            .setInputCols([\"cleanTokens\"]) \\\n",
        "            .setOutputCol(\"stem\")  \n",
        "#3.6.6 Finishing\n",
        "finisher = Finisher() \\\n",
        "              .setInputCols([\"stem\"]) \\\n",
        "              .setOutputCols([\"tokens\"]) \\\n",
        "              .setOutputAsArray(True) \\\n",
        "              .setCleanAnnotations(False)  \n",
        "\n",
        "#3.6.7 Buildthe ML Pipeline\n",
        "nlp_pipeline = Pipeline(stages=[document_assembler, tokenizer,normalizer,stopwords_cleaner, stemmer, finisher]) \n",
        "\n",
        "#3.6.8 Train and Apply the ML Pipeline\n",
        "nlp_model = nlp_pipeline.fit(df)\n",
        "processed_df  = nlp_model.transform(df)\n",
        "tokens_df = processed_df.select('publish_date','tokens').limit(10000)\n",
        "tokens_df.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------+--------------------+\n",
            "|publish_date|              tokens|\n",
            "+------------+--------------------+\n",
            "|    20030219|[aba, decid, comm...|\n",
            "|    20030219|[act, fire, wit, ...|\n",
            "|    20030219|[g, call, infrast...|\n",
            "|    20030219|[air, nz, staff, ...|\n",
            "|    20030219|[air, nz, strike,...|\n",
            "|    20030219|[ambiti, olsson, ...|\n",
            "|    20030219|[antic, delight, ...|\n",
            "|    20030219|[aussi, qualifi, ...|\n",
            "|    20030219|[aust, address, u...|\n",
            "|    20030219|[australia, lock,...|\n",
            "|    20030219|[australia, contr...|\n",
            "|    20030219|[barca, take, rec...|\n",
            "|    20030219|[bathhous, plan, ...|\n",
            "|    20030219|[big, hope, launc...|\n",
            "|    20030219|[big, plan, boost...|\n",
            "|    20030219|[blizzard, buri, ...|\n",
            "|    20030219|[brigadi, dismiss...|\n",
            "|    20030219|[british, combat,...|\n",
            "|    20030219|[bryant, lead, la...|\n",
            "|    20030219|[bushfir, victim,...|\n",
            "+------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjAofbR-j_n8"
      },
      "source": [
        "## 3.7 Feature Engineering\n",
        "\n",
        "We will use Spark MLlib’s CountVectorizer to generate features from textual data. Latent Dirichlet Allocation requires a data specific vocabulary to perform topic modeling."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LC24YcT3ha0O"
      },
      "source": [
        "from pyspark.ml.feature import CountVectorizer\n",
        "cv = CountVectorizer(inputCol=\"tokens\", outputCol=\"features\", vocabSize=500, minDF=3.0)\n",
        "# train the model\n",
        "cv_model = cv.fit(tokens_df)\n",
        "# transform the data. Output column name will be features.\n",
        "vectorized_tokens = cv_model.transform(tokens_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odnjK-aZwgl5"
      },
      "source": [
        "## Build the LDA Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71QJ7lqzhqvv",
        "outputId": "99bcd14e-16f1-48f4-d695-5516506fa620"
      },
      "source": [
        "from pyspark.ml.clustering import LDA\n",
        "num_topics = 3\n",
        "lda = LDA(k=num_topics, maxIter=10)\n",
        "model = lda.fit(vectorized_tokens)\n",
        "ll = model.logLikelihood(vectorized_tokens)\n",
        "lp = model.logPerplexity(vectorized_tokens)\n",
        "print(\"The lower bound on the log likelihood of the entire corpus: \"+ str(ll))\n",
        "print(\"The upper bound on perplexity: \"+ str(lp))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The lower bound on the log likelihood of the entire corpus: -179487.63762961264\n",
            "The upper bound on perplexity: 6.326224363090816\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j62a6XOnwh_t"
      },
      "source": [
        "## Visualize the topics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KSHcIMw8wlUb",
        "outputId": "1f71dd42-5ea3-4f05-df40-f9899ac93902"
      },
      "source": [
        "# extract vocabulary from CountVectorizer\n",
        "vocab = cv_model.vocabulary\n",
        "topics = model.describeTopics()   \n",
        "topics_rdd = topics.rdd\n",
        "topics_words = topics_rdd \\\n",
        "                .map(lambda row: row['termIndices'])\\\n",
        "                .map(lambda idx_list: [vocab[idx] for idx in idx_list]).collect()\n",
        "for idx, topic in enumerate(topics_words):\n",
        "  print(\"topic: {}\".format(idx))\n",
        "  print(\"*\"*25)\n",
        "  for word in topic:\n",
        "    print(word)\n",
        "    print(\"*\"*25)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "topic: 0\n",
            "*************************\n",
            "iraq\n",
            "*************************\n",
            "polic\n",
            "*************************\n",
            "sai\n",
            "*************************\n",
            "council\n",
            "*************************\n",
            "new\n",
            "*************************\n",
            "win\n",
            "*************************\n",
            "crash\n",
            "*************************\n",
            "mai\n",
            "*************************\n",
            "report\n",
            "*************************\n",
            "world\n",
            "*************************\n",
            "topic: 1\n",
            "*************************\n",
            "war\n",
            "*************************\n",
            "plan\n",
            "*************************\n",
            "govt\n",
            "*************************\n",
            "protest\n",
            "*************************\n",
            "iraqi\n",
            "*************************\n",
            "anti\n",
            "*************************\n",
            "water\n",
            "*************************\n",
            "new\n",
            "*************************\n",
            "rain\n",
            "*************************\n",
            "fire\n",
            "*************************\n",
            "topic: 2\n",
            "*************************\n",
            "u\n",
            "*************************\n",
            "man\n",
            "*************************\n",
            "charg\n",
            "*************************\n",
            "call\n",
            "*************************\n",
            "govt\n",
            "*************************\n",
            "get\n",
            "*************************\n",
            "court\n",
            "*************************\n",
            "lead\n",
            "*************************\n",
            "fund\n",
            "*************************\n",
            "baghdad\n",
            "*************************\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_dnfwBcS6eC"
      },
      "source": [
        "## Assignment 5\n",
        "Trying different values of k and maxIter to see which combination best suits our data\n",
        "\n",
        "With k=3 Maxiter 10\n",
        "The lower bound on the log likelihood of the entire corpus: -179487.63762961264\n",
        "The upper bound on perplexity: 6.326224363090816"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5omVTHMki-l",
        "outputId": "2ddfda53-6073-4c62-ab24-dcec2c47b0a8"
      },
      "source": [
        "from pyspark.ml.clustering import LDA\n",
        "num_topics = 2\n",
        "lda = LDA(k=num_topics, maxIter=10)\n",
        "model = lda.fit(vectorized_tokens)\n",
        "ll = model.logLikelihood(vectorized_tokens)\n",
        "lp = model.logPerplexity(vectorized_tokens)\n",
        "print(\"The lower bound on the log likelihood of the entire corpus: \"+ str(ll))\n",
        "print(\"The upper bound on perplexity: \"+ str(lp))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The lower bound on the log likelihood of the entire corpus: -176015.1953842923\n",
            "The upper bound on perplexity: 6.203834603986054\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGhC9bjKkj7P",
        "outputId": "4a47120c-68a5-4d9c-a08b-0fab2f85d35b"
      },
      "source": [
        "from pyspark.ml.clustering import LDA\n",
        "num_topics = 5\n",
        "lda = LDA(k=num_topics, maxIter=15)\n",
        "model = lda.fit(vectorized_tokens)\n",
        "ll = model.logLikelihood(vectorized_tokens)\n",
        "lp = model.logPerplexity(vectorized_tokens)\n",
        "print(\"The lower bound on the log likelihood of the entire corpus: \"+ str(ll))\n",
        "print(\"The upper bound on perplexity: \"+ str(lp))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The lower bound on the log likelihood of the entire corpus: -182818.79801061886\n",
            "The upper bound on perplexity: 6.443634499175908\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJPQJxx7kkhH",
        "outputId": "9636635c-0ad2-478a-acd9-13488ef524d2"
      },
      "source": [
        "from pyspark.ml.clustering import LDA\n",
        "num_topics = 2\n",
        "lda = LDA(k=num_topics, maxIter=15)\n",
        "model = lda.fit(vectorized_tokens)\n",
        "ll = model.logLikelihood(vectorized_tokens)\n",
        "lp = model.logPerplexity(vectorized_tokens)\n",
        "print(\"The lower bound on the log likelihood of the entire corpus: \"+ str(ll))\n",
        "print(\"The upper bound on perplexity: \"+ str(lp))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The lower bound on the log likelihood of the entire corpus: -175191.71152018418\n",
            "The upper bound on perplexity: 6.174810077547729\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCf8dVUGklCM",
        "outputId": "736bce2a-347a-4583-9c88-f5c9e8592205"
      },
      "source": [
        "from pyspark.ml.clustering import LDA\n",
        "num_topics = 3\n",
        "lda = LDA(k=num_topics, maxIter=50)\n",
        "model = lda.fit(vectorized_tokens)\n",
        "ll = model.logLikelihood(vectorized_tokens)\n",
        "lp = model.logPerplexity(vectorized_tokens)\n",
        "print(\"The lower bound on the log likelihood of the entire corpus: \"+ str(ll))\n",
        "print(\"The upper bound on perplexity: \"+ str(lp))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The lower bound on the log likelihood of the entire corpus: -175926.4439257532\n",
            "The upper bound on perplexity: 6.2007064685518545\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NU9jP-tUkldB",
        "outputId": "f925d0f6-63c6-4442-f844-681c7fadf389"
      },
      "source": [
        "from pyspark.ml.clustering import LDA\n",
        "num_topics = 7\n",
        "lda = LDA(k=num_topics, maxIter=15)\n",
        "model = lda.fit(vectorized_tokens)\n",
        "ll = model.logLikelihood(vectorized_tokens)\n",
        "lp = model.logPerplexity(vectorized_tokens)\n",
        "print(\"The lower bound on the log likelihood of the entire corpus: \"+ str(ll))\n",
        "print(\"The upper bound on perplexity: \"+ str(lp))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The lower bound on the log likelihood of the entire corpus: -186713.43285278097\n",
            "The upper bound on perplexity: 6.580904865810693\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "poqMPgY8lzzr"
      },
      "source": [
        "## Observation:\n",
        "\n",
        "The model with k = 2 and maxIter = 15 looks to be best as it has minimum perplexity of all and maximum likelihood. Also, I observed that with the decrease in k and increase in maximum iterations likelihood increased and perplexity decreased"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbt8d2-Ax90X"
      },
      "source": [
        "## Assignment 6\n",
        "\n",
        "Rewrite the codes for finding topics in tweets coronavirus dataset.Try different values of k and maxIter to see which combination best suits the data.Show at least five different combinations, show their results, and explain why it’s best."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNYmx_NLos1e"
      },
      "source": [
        "### Read the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xUFrC0lFTR7v",
        "outputId": "b09ec8ba-6341-4992-c479-bf6e37d84b69"
      },
      "source": [
        "# if you are reading file from local storage \n",
        "file_location = r'/content/coronavirus-text-only-1000.txt'\n",
        "# CSV options\n",
        "infer_schema = \"true\"\n",
        "first_row_is_header = \"true\"\n",
        "cv_df = spark.read.format(\"csv\")\\\n",
        "      .option(\"inferSchema\", infer_schema)\\\n",
        "      .option(\"header\", first_row_is_header)\\\n",
        "      .load(file_location)\n",
        "# Verify the count\n",
        "cv_df.count()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "999"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcSMIc3boxw1"
      },
      "source": [
        "### Preprocess the pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "drpSWmkjTE_P",
        "outputId": "fd15e563-f684-4f75-8076-3dd444bf9c61"
      },
      "source": [
        "#3.6.1Document Assembling\n",
        "document_assembler = DocumentAssembler() \\\n",
        "                      .setInputCol(\"text\") \\\n",
        "                      .setOutputCol(\"document\") \\\n",
        "                      .setCleanupMode(\"shrink\")\n",
        "\n",
        "#Split sentence to tokens(array)\n",
        "tokenizer = Tokenizer() \\\n",
        "              .setInputCols([\"document\"]) \\\n",
        "              .setOutputCol(\"token\")\n",
        "\n",
        "#3.6.3 Normalizing-Clean unwanted characters and garbage\n",
        "normalizer = Normalizer() \\\n",
        "              .setInputCols([\"token\"]) \\\n",
        "              .setOutputCol(\"normalized\")\n",
        "\n",
        "#3.6.4 Stopwords removal\n",
        "stopwords_cleaner = StopWordsCleaner()\\\n",
        "                      .setInputCols(\"token\")\\\n",
        "                      .setOutputCol(\"cleanTokens\")\\\n",
        "                      .setCaseSensitive(False) \\\n",
        "                      .setStopWords([\"coronavirus\"])\n",
        "\n",
        "#3.6.5 Stemming\n",
        "stemmer = Stemmer() \\\n",
        "            .setInputCols([\"cleanTokens\"]) \\\n",
        "            .setOutputCol(\"stem\")  \n",
        "\n",
        "#3.6.6 Finishing\n",
        "finisher = Finisher() \\\n",
        "              .setInputCols([\"stem\"]) \\\n",
        "              .setOutputCols([\"tokens\"]) \\\n",
        "              .setOutputAsArray(True) \\\n",
        "              .setCleanAnnotations(False)  \n",
        "\n",
        "#3.6.7 Build the ML Pipeline\n",
        "nlp_pipeline = Pipeline(stages=[document_assembler, tokenizer,normalizer,stopwords_cleaner, stemmer, finisher]) \n",
        "\n",
        "# 3.6.8 Train and Apply the ML Pipeline\n",
        "nlp_model = nlp_pipeline.fit(cv_df)\n",
        "processed_cvdf  = nlp_model.transform(cv_df)\n",
        "tokens_cvdf = processed_cvdf.select('text','tokens').limit(10000)\n",
        "tokens_cvdf.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+\n",
            "|                text|              tokens|\n",
            "+--------------------+--------------------+\n",
            "|Studies look at t...|[studi, look, at,...|\n",
            "|RT @EricTopol: Th...|[rt, @erictopol, ...|\n",
            "|RT @NPR: Working ...|[rt, @npr, :, wor...|\n",
            "|\"RT @Harvey_Walke...|[\", rt, @harvey_w...|\n",
            "|RT @CNNEE: La far...|[rt, @cnnee, :, l...|\n",
            "|RT @ReutersWorld:...|[rt, @reutersworl...|\n",
            "|RT @CNN: This Ill...|[rt, @cnn, :, thi...|\n",
            "|\"RT @Censelio: Ar...|[\", rt, @censelio...|\n",
            "|RT @jilevin: Trum...|[rt, @jilevin, :,...|\n",
            "|RT @propublica: P...|[rt, @propublica,...|\n",
            "|NSW to close Vict...|[nsw, to, close, ...|\n",
            "|RT @ASlavitt: Tru...|[rt, @aslavitt, :...|\n",
            "|RT @ClayTravis: C...|[rt, @claytravi, ...|\n",
            "|RT @JamesGunn: I'...|[rt, @jamesgunn, ...|\n",
            "|RT @NatashaFatah:...|[rt, @natashafata...|\n",
            "|RT @crissles: Y‚Ä...|[rt, @crissl, :, ...|\n",
            "|\"RT @Censelio: Ar...|[\", rt, @censelio...|\n",
            "|RT @Villarruel_cl...|[rt, @villarruel_...|\n",
            "|RT @JaxAlemany: ‚...|[rt, @jaxalemani,...|\n",
            "|RT @JamesGunn: I'...|[rt, @jamesgunn, ...|\n",
            "+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfQ0ofJGpGp1"
      },
      "source": [
        "### Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXH0kohdhxDT"
      },
      "source": [
        "from pyspark.ml.feature import CountVectorizer\n",
        "cv = CountVectorizer(inputCol=\"tokens\", outputCol=\"features\", vocabSize=500, minDF=3.0)\n",
        "# train the model\n",
        "cv_model = cv.fit(tokens_cvdf)\n",
        "# transform the data. Output column name will be features.\n",
        "vectorized_tokens = cv_model.transform(tokens_cvdf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLouM-EhpJa2"
      },
      "source": [
        "### Build LDA model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbYFpBsIh2Hy",
        "outputId": "3fd8b041-d197-40ee-a904-f1294dcb2bc7"
      },
      "source": [
        "from pyspark.ml.clustering import LDA\n",
        "num_topics = 3\n",
        "lda = LDA(k=num_topics, maxIter=10)\n",
        "model = lda.fit(vectorized_tokens)\n",
        "ll = model.logLikelihood(vectorized_tokens)\n",
        "lp = model.logPerplexity(vectorized_tokens)\n",
        "print(\"The lower bound on the log likelihood of the entire corpus: \"+ str(ll))\n",
        "print(\"The upper bound on perplexity: \"+ str(lp))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The lower bound on the log likelihood of the entire corpus: -84381.97508253767\n",
            "The upper bound on perplexity: 5.28311890073489\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNt-Jn16pQQF"
      },
      "source": [
        "### Visualize topics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Br77M494h2yo",
        "outputId": "a15a90ad-c74d-4c94-ad77-470f9c0ee388"
      },
      "source": [
        "# extract vocabulary from CountVectorizer\n",
        "vocab = cv_model.vocabulary\n",
        "topics = model.describeTopics()   \n",
        "topics_rdd = topics.rdd\n",
        "topics_words = topics_rdd \\\n",
        "                .map(lambda row: row['termIndices'])\\\n",
        "                .map(lambda idx_list: [vocab[idx] for idx in idx_list]).collect()\n",
        "for idx, topic in enumerate(topics_words):\n",
        "  print(\"topic: {}\".format(idx))\n",
        "  print(\"*\"*25)\n",
        "  for word in topic:\n",
        "    print(word)\n",
        "    print(\"*\"*25)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "topic: 0\n",
            "*************************\n",
            ":\n",
            "*************************\n",
            "rt\n",
            "*************************\n",
            ",\n",
            "*************************\n",
            ".\n",
            "*************************\n",
            "the\n",
            "*************************\n",
            "de\n",
            "*************************\n",
            "to\n",
            "*************************\n",
            "a\n",
            "*************************\n",
            "la\n",
            "*************************\n",
            "and\n",
            "*************************\n",
            "topic: 1\n",
            "*************************\n",
            ".\n",
            "*************************\n",
            "in\n",
            "*************************\n",
            "the\n",
            "*************************\n",
            ":\n",
            "*************************\n",
            "rt\n",
            "*************************\n",
            "have\n",
            "*************************\n",
            "a\n",
            "*************************\n",
            ";\n",
            "*************************\n",
            "&amp\n",
            "*************************\n",
            "over\n",
            "*************************\n",
            "topic: 2\n",
            "*************************\n",
            ":\n",
            "*************************\n",
            "rt\n",
            "*************************\n",
            "a\n",
            "*************************\n",
            "the\n",
            "*************************\n",
            ".\n",
            "*************************\n",
            ",\n",
            "*************************\n",
            "she\n",
            "*************************\n",
            "than\n",
            "*************************\n",
            "trump\n",
            "*************************\n",
            "do\n",
            "*************************\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qh8NYJqpu71"
      },
      "source": [
        "### Modelling with various k and maxIter values\n",
        "For k=3; maxIter=10,\n",
        "The lower bound on the log likelihood of the entire corpus: -84381.97508253767\n",
        "The upper bound on perplexity: 5.28311890073489"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kBgzCa_oqNzo",
        "outputId": "7e2a390c-dc81-4ef3-eca6-c310d90d3a67"
      },
      "source": [
        "from pyspark.ml.clustering import LDA\n",
        "num_topics = 2\n",
        "lda = LDA(k=num_topics, maxIter=15)\n",
        "model = lda.fit(vectorized_tokens)\n",
        "ll = model.logLikelihood(vectorized_tokens)\n",
        "lp = model.logPerplexity(vectorized_tokens)\n",
        "print(\"The lower bound on the log likelihood of the entire corpus: \"+ str(ll))\n",
        "print(\"The upper bound on perplexity: \"+ str(lp))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The lower bound on the log likelihood of the entire corpus: -84045.42245923339\n",
            "The upper bound on perplexity: 5.262047486803994\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9jVSUA0Cqc8f",
        "outputId": "8975018b-70b0-40f4-bbac-0e242cd97831"
      },
      "source": [
        "from pyspark.ml.clustering import LDA\n",
        "num_topics = 2\n",
        "lda = LDA(k=num_topics, maxIter=50)\n",
        "model = lda.fit(vectorized_tokens)\n",
        "ll = model.logLikelihood(vectorized_tokens)\n",
        "lp = model.logPerplexity(vectorized_tokens)\n",
        "print(\"The lower bound on the log likelihood of the entire corpus: \"+ str(ll))\n",
        "print(\"The upper bound on perplexity: \"+ str(lp))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The lower bound on the log likelihood of the entire corpus: -83015.46169817106\n",
            "The upper bound on perplexity: 5.197562089792829\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FR9c9vTqqjoI",
        "outputId": "0b88dee6-8591-42fd-84cc-d65b4a24706f"
      },
      "source": [
        "from pyspark.ml.clustering import LDA\n",
        "num_topics = 3\n",
        "lda = LDA(k=num_topics, maxIter=100)\n",
        "model = lda.fit(vectorized_tokens)\n",
        "ll = model.logLikelihood(vectorized_tokens)\n",
        "lp = model.logPerplexity(vectorized_tokens)\n",
        "print(\"The lower bound on the log likelihood of the entire corpus: \"+ str(ll))\n",
        "print(\"The upper bound on perplexity: \"+ str(lp))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The lower bound on the log likelihood of the entire corpus: -81105.92844496891\n",
            "The upper bound on perplexity: 5.078007040130786\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Rwog0Neql9t",
        "outputId": "933ebfa2-08d7-4214-ab74-46dd0cf441cf"
      },
      "source": [
        "from pyspark.ml.clustering import LDA\n",
        "num_topics = 5\n",
        "lda = LDA(k=num_topics, maxIter=50)\n",
        "model = lda.fit(vectorized_tokens)\n",
        "ll = model.logLikelihood(vectorized_tokens)\n",
        "lp = model.logPerplexity(vectorized_tokens)\n",
        "print(\"The lower bound on the log likelihood of the entire corpus: \"+ str(ll))\n",
        "print(\"The upper bound on perplexity: \"+ str(lp))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The lower bound on the log likelihood of the entire corpus: -80504.5831028166\n",
            "The upper bound on perplexity: 5.040357068796431\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rN8pQAfWqml8",
        "outputId": "39561d26-b6b0-48bc-da68-ab8c0e1aa164"
      },
      "source": [
        "from pyspark.ml.clustering import LDA\n",
        "num_topics = 7\n",
        "lda = LDA(k=num_topics, maxIter=30)\n",
        "model = lda.fit(vectorized_tokens)\n",
        "ll = model.logLikelihood(vectorized_tokens)\n",
        "lp = model.logPerplexity(vectorized_tokens)\n",
        "print(\"The lower bound on the log likelihood of the entire corpus: \"+ str(ll))\n",
        "print(\"The upper bound on perplexity: \"+ str(lp))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The lower bound on the log likelihood of the entire corpus: -84764.05701580833\n",
            "The upper bound on perplexity: 5.307040885036835\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12UGR4lyrF3v"
      },
      "source": [
        "## Observation\n",
        "\n",
        "For this dataset, k=5 and maxIter=50 would be best parameters as the likelihood is more and perplexity is less of all combinations."
      ]
    }
  ]
}